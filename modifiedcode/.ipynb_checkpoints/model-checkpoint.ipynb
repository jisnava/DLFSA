{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "helixCord=[]\n",
    "sheetCord=[]\n",
    "coilCord=[]\n",
    "\n",
    "patt=re.compile('(ATOM)\\s+([0-9]+)\\s+(CA)\\s+([A-Z]+)\\s+([A-Z]{1})\\s+([0-9]+)\\s+([-+]?\\d+\\.\\d+)\\s+([-+]?\\d+\\.\\d+)\\s+([-+]?\\d+\\.\\d+)\\s+([-+]?\\d+\\.\\d+)\\s+([-+]?\\d+\\.\\d+)\\s+', flags=re.S)\n",
    "\n",
    "amino_acid_id={'ALA':0, 'CYS':1, 'ASP':2, 'GLU':3, 'PHE':4, 'GLY':5, 'HIS':6, 'ILE':7, 'LYS':8, 'LEU':9, 'MET':10,\n",
    "              'ASN':11, 'PRO':12, 'GLN':13, 'ARG':14, 'SER':15, 'THR':16, 'VAL':17, 'TRP':18, 'TYR':19}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_files(path): #opens each file and extract coordinates of CA+extra 2 values and stores in temp\n",
    "    arr=[]\n",
    "\n",
    "    for file in glob.glob(path, recursive=True):\n",
    "        temp=[]\n",
    "        with open(file) as fi:\n",
    "            try:\n",
    "                for results in patt.findall(fi.read()):\n",
    "                    val=results[3:9]\n",
    "                    val_=list(val)\n",
    "                    del val_[2]\n",
    "                    del val_[1]\n",
    "                    if len(val_[0])!=3: #for replacing 4word acid name with 3word\n",
    "                        name=val_[0][1:]\n",
    "                        val_[0]=name\n",
    "                    acid_id=amino_acid_id[val_[0]]\n",
    "                    val_[0]=acid_id\n",
    "                    val_final=[float(x) for x in val_]\n",
    "                    temp.append(val_final)\n",
    "            except UnicodeDecodeError:\n",
    "                os.remove(file)\n",
    "                #check.append(results)\n",
    "    \n",
    "        arr.append(temp)    # finally append temp array which contains details of CA of a single file \n",
    "                                #into another array\n",
    "   \n",
    "    return arr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reads inputs\n",
    "helixCord=get_files('sample/helix/*')\n",
    "sheetCord=get_files('sample/sheet/*')\n",
    "coilCord=get_files('sample/coil/*')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted(arr): #remove array with lengths other than given in list\n",
    "    for val in arr:\n",
    "        if len(val) not in [9,6,5,3]:\n",
    "            arr.remove(val)\n",
    "    arr_=[np.array(val, dtype=np.float32) for val in arr]\n",
    "    arr_=np.array(arr_)\n",
    "    return arr_\n",
    "\n",
    "\n",
    "def pad(arr):     # pads array with zero\n",
    "    final=[]\n",
    "    for val in arr:\n",
    "        zeros=np.zeros((9,4))\n",
    "        zeros[:val.shape[0], :val.shape[1]]=val\n",
    "        final.append(zeros)\n",
    "    return final\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helixfinal=[]\n",
    "sheetfinal=[]\n",
    "coilfinal=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "helixfinal=remove_unwanted(helixCord)\n",
    "sheetfinal=remove_unwanted(sheetCord)\n",
    "coilfinal=remove_unwanted(coilCord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39485\n",
      "39905\n",
      "39837\n"
     ]
    }
   ],
   "source": [
    "print(len(helixfinal))\n",
    "print(len(sheetfinal))\n",
    "print(len(coilfinal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "helixfinal=pad(helixfinal)\n",
    "sheetfinal=pad(sheetfinal)\n",
    "coilfinal=pad(coilfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for verification purpose..\n",
    "\n",
    "from collections import Counter\n",
    "sam=[]\n",
    "for i in coilfinal:\n",
    "    sam.append(i.shape)\n",
    "    \n",
    "print(Counter(sam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stores no. of input values of each shapes (for preparing labels)\n",
    "\n",
    "num_sheet=np.array(sheetfinal).shape[0]\n",
    "num_helix=np.array(helixfinal).shape[0]\n",
    "num_coil=np.array(coilfinal).shape[0]\n",
    "\n",
    "#scales array values between -1 and 1\n",
    "\n",
    "sheetfinal_=np.array([2.*(a - np.min(a))/np.ptp(a)-1 for a in sheetfinal])\n",
    "helixfinal_=np.array([2.*(a - np.min(a))/np.ptp(a)-1 for a in helixfinal])\n",
    "coilfinal_=np.array([2.*(a - np.min(a))/np.ptp(a)-1 for a in coilfinal])\n",
    "\n",
    "features1=np.concatenate((sheetfinal_,helixfinal_, coilfinal_), axis=0) #concatenate each array into single one\n",
    "features_=np.array([arr[:, :, np.newaxis] for arr in features1]) #adds extra axis to mention depth of array\n",
    "\n",
    "#labels creation..\n",
    "# index 0 - sheet\n",
    "#index 1 - helix\n",
    "#index 2 - coil\n",
    "labels1=np.concatenate((np.array([[1.,0.,0.]*num_sheet]), np.array([[0.,1.,0.]*num_helix]), np.array([[0.,0.,1.]*num_coil]) ), axis=1 )\n",
    "labels_=np.reshape(labels1, (num_sheet+num_helix+num_coil,3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119218\n",
      "119218\n"
     ]
    }
   ],
   "source": [
    "print(len(features_)) #for verifying features and labels are of same length..\n",
    "print(len(labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly shuffle the dataset\n",
    "from random import shuffle\n",
    "idx=[i for i in range(len(features_))]\n",
    "shuffle(idx)\n",
    "features_=features_[idx, :, :, :]\n",
    "labels_=labels_[idx, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train, test and validation\n",
    "#test dataset consist of 10% total \n",
    "#valid dataset consist of 10-20 % of train dataset\n",
    "\n",
    "features=features_[:109218]\n",
    "labels=labels_[:109218]\n",
    "\n",
    "test_feat=features_[109218:]\n",
    "test_lab=labels_[109218:]\n",
    "\n",
    "valid_feat=features_[:21843]\n",
    "valid_labels=labels_[:21843]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "epochs=200\n",
    "batch_size=512\n",
    "display_step=10\n",
    "num_classes=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 9, 4, 16)\n",
      "(?, 9, 4, 16)\n",
      "(?, 9, 4, 32)\n",
      "(?, 8, 3, 32)\n",
      "(?, 8, 3, 64)\n",
      "(?, 8, 3, 64)\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "conv2d/kernel:0 (float32_ref 2x2x1x16) [64, bytes: 256]\n",
      "conv2d/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv2d_1/kernel:0 (float32_ref 2x2x16x16) [1024, bytes: 4096]\n",
      "conv2d_1/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv2d_2/kernel:0 (float32_ref 2x2x16x32) [2048, bytes: 8192]\n",
      "conv2d_2/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv2d_3/kernel:0 (float32_ref 2x2x32x64) [8192, bytes: 32768]\n",
      "conv2d_3/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "conv2d_4/kernel:0 (float32_ref 2x2x64x64) [16384, bytes: 65536]\n",
      "conv2d_4/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "conv2d_5/kernel:0 (float32_ref 2x2x64x64) [16384, bytes: 65536]\n",
      "conv2d_5/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "fully_connected/weights:0 (float32_ref 1536x128) [196608, bytes: 786432]\n",
      "fully_connected/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "fully_connected_1/weights:0 (float32_ref 128x256) [32768, bytes: 131072]\n",
      "fully_connected_1/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "fully_connected_2/weights:0 (float32_ref 256x512) [131072, bytes: 524288]\n",
      "fully_connected_2/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "fully_connected_3/weights:0 (float32_ref 512x1024) [524288, bytes: 2097152]\n",
      "fully_connected_3/biases:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "fully_connected_4/weights:0 (float32_ref 256x3) [768, bytes: 3072]\n",
      "fully_connected_4/biases:0 (float32_ref 3) [3, bytes: 12]\n",
      "Total size of variables: 931779\n",
      "Total bytes of variables: 3727116\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #for resetting current graph\n",
    "train_graph=tf.Graph()\n",
    "\n",
    "def get_summary(vars_):\n",
    "    tf.contrib.slim.model_analyzer.analyze_vars(vars_, print_info=True)\n",
    "\n",
    "#def model(input_, num_classes, dropout): #model (CNN) definition    \n",
    "with train_graph.as_default():\n",
    "\n",
    "    x=tf.placeholder(dtype=tf.float32,shape=[None, 9, 4,1])\n",
    "    y=tf.placeholder(dtype=tf.float32, shape=[None, 3])\n",
    "    keep_prob=tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "    layer0=tf.layers.conv2d(inputs=x, filters=16, kernel_size=2, padding='SAME',\n",
    "                                    activation=tf.nn.relu)\n",
    "    print(layer0.shape)\n",
    "    layer1=tf.layers.conv2d(inputs=layer0, filters=16, kernel_size=2, padding='SAME',\n",
    "                                    activation=tf.nn.relu)\n",
    "    print(layer1.shape)\n",
    "    layer2=tf.layers.conv2d(inputs=layer1, filters=32, kernel_size=2, padding='SAME',\n",
    "                                    activation=tf.nn.relu)\n",
    "    print(layer2.shape)\n",
    "    layer2=tf.layers.max_pooling2d(layer2, strides=1, pool_size=2)\n",
    "    print(layer2.shape)\n",
    "    layer3=tf.layers.conv2d(inputs=layer2, filters=64, kernel_size=2, padding='SAME',\n",
    "                                    activation=tf.nn.relu)\n",
    "    print(layer3.shape)\n",
    "    layer4=tf.layers.conv2d(inputs=layer3, filters=64, kernel_size=2, padding='SAME',\n",
    "                                    activation=tf.nn.relu)\n",
    "    layer5=tf.layers.conv2d(inputs=layer4, filters=64, kernel_size=2, padding='SAME',\n",
    "                                    activation=tf.nn.relu)\n",
    "\n",
    "    #layer4=tf.contrib.layers.max_pool2d(layer4, 2)\n",
    "    print(layer4.shape)\n",
    "\n",
    "    fc1=tf.contrib.layers.flatten(layer5)\n",
    "    \n",
    "    fc1=tf.contrib.layers.fully_connected(fc1,128)\n",
    "    fc1=tf.contrib.layers.dropout(fc1, keep_prob=keep_prob, is_training=True)\n",
    "\n",
    "    fc2=tf.contrib.layers.fully_connected(fc1,256)\n",
    "    fc2=tf.contrib.layers.dropout(fc2, keep_prob=keep_prob, is_training=True)\n",
    "    fc3=tf.contrib.layers.fully_connected(fc2,512)\n",
    "    fc4=tf.contrib.layers.fully_connected(fc3,1024)\n",
    "    fc4=tf.contrib.layers.dropout(fc4, keep_prob=keep_prob, is_training=True)\n",
    "\n",
    "        #drop=tf.contrib.layers.dropout(fc2, keep_prob=dropout, is_training=True)\n",
    "        #fc3=tf.contrib.layers.fully_connected(fc2,512)\n",
    "        #fc4=tf.contrib.layers.fully_connected(fc3,1024)\n",
    "        #drop=tf.contrib.layers.dropout(fc4, keep_prob=dropout, is_training=True)\n",
    "    logits=tf.contrib.layers.fully_connected(fc2, num_outputs=num_classes, activation_fn=tf.nn.softmax)\n",
    "    loss=tf.losses.softmax_cross_entropy(y, logits)#mention loss function\n",
    "    optm=tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)#mention optimizer with learning rate\n",
    "    correctPred=tf.equal(tf.argmax(logits,1), tf.argmax(y,1))# an array which stores 1 for each correct prediction and                                                    # 0 for wrong prediction \n",
    "    acc=tf.reduce_mean(tf.cast(correctPred, tf.float32)) # acc calculated by taking mean of above array\n",
    "\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "    \n",
    "\n",
    "    saver=tf.train.Saver()\n",
    "    \n",
    "    save_file='./training_logs11.ckpt'\n",
    "    model_vars=tf.trainable_variables()\n",
    "    get_summary(model_vars)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(features, labels, batch_size):\n",
    "    out_batches=[]\n",
    "    \n",
    "    for start in range(0,len(features),batch_size):\n",
    "        end=start+batch_size\n",
    "        batches=[features[start:end], labels[start:end]]\n",
    "        out_batches.append(batches)\n",
    "    \n",
    "    return out_batches\n",
    "\n",
    "def epoch_stats(sess, epoch, last_feat, last_lab, valid_features,valid_labels, trn_loss, merge, train_writer2,train_writer3, counter):\n",
    "    \n",
    "    summary, cur_cost=sess.run([merge, loss], feed_dict={x:last_feat, y:last_lab,keep_prob:0.6})\n",
    "    train_writer2.add_summary(summary, counter)\n",
    "    summ, cur_Acc=sess.run([merge, acc], feed_dict={x:valid_features, y:valid_labels, keep_prob:0.6})\n",
    "    train_writer3.add_summary(summ, counter)\n",
    "    print('epoch : {:<4} - train_loss : {:<8.3} - val_loss : {:<8.3} - acc: {:<5.3}'.format(epoch, trn_loss, cur_cost, cur_Acc) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0    - train_loss : 0.992    - val_loss : 0.975    - acc: 0.532\n",
      "epoch : 10   - train_loss : 0.783    - val_loss : 0.765    - acc: 0.742\n",
      "epoch : 20   - train_loss : 0.727    - val_loss : 0.717    - acc: 0.79 \n",
      "epoch : 30   - train_loss : 0.69     - val_loss : 0.689    - acc: 0.802\n",
      "epoch : 40   - train_loss : 0.671    - val_loss : 0.672    - acc: 0.818\n",
      "epoch : 50   - train_loss : 0.653    - val_loss : 0.648    - acc: 0.835\n",
      "epoch : 60   - train_loss : 0.662    - val_loss : 0.654    - acc: 0.84 \n",
      "epoch : 70   - train_loss : 0.644    - val_loss : 0.643    - acc: 0.843\n",
      "epoch : 80   - train_loss : 0.629    - val_loss : 0.637    - acc: 0.845\n",
      "epoch : 90   - train_loss : 0.659    - val_loss : 0.659    - acc: 0.858\n",
      "epoch : 100  - train_loss : 0.628    - val_loss : 0.62     - acc: 0.858\n",
      "epoch : 110  - train_loss : 0.616    - val_loss : 0.612    - acc: 0.856\n",
      "epoch : 120  - train_loss : 0.62     - val_loss : 0.616    - acc: 0.874\n",
      "epoch : 130  - train_loss : 0.609    - val_loss : 0.607    - acc: 0.878\n",
      "epoch : 140  - train_loss : 0.626    - val_loss : 0.629    - acc: 0.875\n",
      "epoch : 150  - train_loss : 0.64     - val_loss : 0.636    - acc: 0.87 \n",
      "epoch : 160  - train_loss : 0.615    - val_loss : 0.613    - acc: 0.879\n",
      "epoch : 170  - train_loss : 0.618    - val_loss : 0.619    - acc: 0.882\n",
      "epoch : 180  - train_loss : 0.613    - val_loss : 0.621    - acc: 0.876\n",
      "epoch : 190  - train_loss : 0.615    - val_loss : 0.604    - acc: 0.881\n",
      "0.825\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_writer1=tf.summary.FileWriter('summary/train_loss', train_graph)\n",
    "    train_writer2=tf.summary.FileWriter('summary/valid_loss', train_graph)\n",
    "    train_writer3=tf.summary.FileWriter('summary/', train_graph)\n",
    "\n",
    "    merge=tf.summary.merge_all()\n",
    "    counter=0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for values in get_batches(features, labels, batch_size):\n",
    "            train_feat=values[0]\n",
    "            train_lab=values[1]\n",
    "            counter+=1\n",
    "            summ, loss_, _ = sess.run([merge, loss, optm], feed_dict={x:train_feat, y:train_lab, keep_prob:0.6})# runs optimizer on input features\n",
    "            train_writer1.add_summary(summ, counter)\n",
    "        if (e%display_step==0):\n",
    "            epoch_stats(sess, e, train_feat, train_lab, valid_feat, valid_labels, loss_, merge, train_writer2,train_writer3,counter)\n",
    "            \n",
    "    model_acc=sess.run(acc, feed_dict={x:test_feat, y:test_lab, keep_prob:0.6})\n",
    "    print(model_acc)\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"87/80 keep prob- 0.6 for train, 0.7 for valid\"\n",
    "\"86/80 log10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-01d2468ac1f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loads sample test cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtestcases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's_93.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtestcases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestcases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestcases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-35fb91e67b99>\u001b[0m in \u001b[0;36mget_files\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                     \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mval_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'patt' is not defined"
     ]
    }
   ],
   "source": [
    "#loads sample test cases\n",
    "\n",
    "testcases=get_files('s_93.txt')\n",
    "testcases=[np.array(i) for i in testcases]\n",
    "padded=pad(testcases)\n",
    "inputs=np.array([2.*(a - np.min(a))/np.ptp(a)-1 for a in padded])\n",
    "inputs_=np.array([arr[:, :, np.newaxis] for arr in inputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3377654c8109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_graph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./training_logs11.ckpt.meta'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#restore trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#sample test cases are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_graph' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph('./training_logs11.ckpt.meta') #restore trained weights\n",
    "    loader.restore(sess, save_file) \n",
    "    predict=[]\n",
    "    predict.append(sess.run( tf.argmax(logits, 1), feed_dict={x:inputs_, keep_prob:0.7}))#sample test cases are \n",
    "                                                                            #given here\n",
    "    \n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
